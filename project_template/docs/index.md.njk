# ${{ values.repository_name }}

Note: this is a copy of README.md

This ETL Pipeline project is designed to extract, transform, and load data using Apache Airflow for orchestration. The project follows best practices in software engineering, including a well-defined structure, linting, security measures, and testing.

## Project Structure

```
etl-pipeline
├── dags
│   └── etl_workflow.py          # Airflow DAG for orchestrating ETL tasks
├── etl
│   ├── extract.py               # Module for data extraction
│   ├── transform.py             # Module for data transformation
│   ├── load.py                  # Module for loading data
│   └── __init__.py              # Package initialization
├── tests
│   ├── unit
│   │   ├── test_extract.py      # Unit tests for data extraction
│   │   ├── test_transform.py     # Unit tests for data transformation
│   │   └── test_load.py         # Unit tests for data loading
│   └── e2e
│       └── test_etl_workflow.py # End-to-end tests for the ETL workflow
├── scripts
│   └── run_etl.py               # Script to run the ETL process manually
├── requirements.txt              # Production dependencies
├── requirements-dev.txt          # Development dependencies
├── .pre-commit-config.yaml       # Pre-commit hooks configuration
├── .flake8                       # Flake8 configuration for linting
├── .gitignore                    # Git ignore file
├── Makefile                      # Makefile for project commands
├── Dockerfile                    # Dockerfile for containerization
└── README.md                     # Project documentation
```

## Setup Instructions

1. **Clone the Repository**

   ```bash
   git clone <repository-url>
   cd ${{ values.repository_name }}
   ```

2. **Install Dependencies**

   - For production:
     ```bash
     pip install -r requirements.txt
     ```
   - For development:
     ```bash
     pip install -r requirements-dev.txt
     ```

3. **Set Up Pre-commit Hooks**

   ```bash
   pre-commit install
   ```

4. **Run the ETL Process**
   You can run the ETL process manually using the provided script:

   ```bash
   python scripts/run_etl.py
   ```

5. **Run Tests**
   - Unit tests:
     ```bash
     pytest tests/unit
     ```
   - End-to-end tests:
     ```bash
     pytest tests/e2e
     ```

## How to Test and Run This Project

### 1. Install Dependencies

```bash
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
pip install -r requirements-dev.txt
```

### 2. Set Up Pre-commit Hooks

```bash
pre-commit install
```

### 3. Linting and Security Checks

```bash
make lint
make security
```

### 4. Run Unit and End-to-End Tests

```bash
make test
```

### 5. Run Airflow and the ETL DAG

#### a. Copy Your DAG to Airflow's DAGs Folder

```bash
mkdir -p ~/airflow/dags
cp dags/etl_workflow.py ~/airflow/dags/
```

#### b. Start Airflow

```bash
airflow standalone
```

- The Airflow UI will be available at [http://localhost:8080](http://localhost:8080).
- Log in with the credentials shown in your terminal or in `~/airflow/simple_auth_manager_passwords.json.generated`.

#### c. Trigger the DAG

- In the Airflow UI, find `etl_workflow`.
- Toggle it "On" and click the "Play" button to trigger a run.
- Click on each task to view logs and confirm successful execution.

### 6. (Optional) Run the ETL Script Manually

```bash
python scripts/run_etl.py
```

---

**Note:**

- If you change your ETL code, restart Airflow to reload the DAG.
- For troubleshooting, check the Airflow UI for import errors or task logs.

## Usage

The ETL pipeline consists of three main components:

- **Extract**: The `extract_data` function retrieves data from the source.
- **Transform**: The `transform_data` function processes the extracted data.
- **Load**: The `load_data` function loads the transformed data into the target destination.

The Airflow DAG defined in `dags/etl_workflow.py` orchestrates these tasks, ensuring they run in the correct order.

## Best Practices

- **Code Quality**: The project uses Flake8 for linting and pre-commit hooks to enforce code quality checks.
- **Testing**: Comprehensive unit and end-to-end tests are included to ensure the reliability of the ETL process.
- **Documentation**: Clear documentation is provided to facilitate understanding and usage of the project.

## License

This project is licensed under the MIT License. See the LICENSE file for more details.
